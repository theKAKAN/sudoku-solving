{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the tf_cnn_model\n",
    "This file is used to train the tf_cnn_model and save it as a .keras zip archive, which can be loaded and reused as required later on.  \n",
    "\n",
    "The libraries cell are common between the two for ease of development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "import cv2\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import load_img\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from pathlib import Path\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data points = 10160\n"
     ]
    }
   ],
   "source": [
    "#Loading the data \n",
    "\n",
    "data = os.listdir(r\"./input/Digits/Digits\" )\n",
    "data_X = []     \n",
    "data_y = []  \n",
    "num_classes = len(data)\n",
    "for i in range (0,num_classes):\n",
    "    data_list = os.listdir(r\"./input/Digits/Digits\" +\"/\"+str(i))\n",
    "    for j in data_list:\n",
    "        pic = cv2.imread(r\"./input/Digits/Digits\" +\"/\"+str(i)+\"/\"+j)\n",
    "        pic = cv2.resize(pic,(32,32))\n",
    "        data_X.append(pic)\n",
    "        data_y.append(i)\n",
    "\n",
    "         \n",
    "if len(data_X) == len(data_y) :\n",
    "    print(\"Total Data points =\",len(data_X))\n",
    "\n",
    "# Labels and images\n",
    "data_X = np.array(data_X)\n",
    "data_y = np.array(data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING DATASET\n",
    "\n",
    "Splitting the dataset into test, train and validation sets. Preprocessing for the features (images) into grayscale, enhancing it with histogram equalization and then normalizing. Followed by converting then into a NumPy array. further reshaping the image's array and using data augmentation. Preprocessing for the labels involves one-hot encoding the label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape =  (7721, 32, 32, 3)\n",
      "Validation Set Shape =  (1931, 32, 32, 3)\n",
      "Test Set Shape =  (508, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#Spliting the train validation and test sets\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_X,data_y,test_size=0.05)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X,train_y,test_size=0.2)\n",
    "print(\"Training Set Shape = \",train_X.shape)\n",
    "print(\"Validation Set Shape = \",valid_X.shape)\n",
    "print(\"Test Set Shape = \",test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the images for neuralnet\n",
    "\n",
    "def PreprocessImage(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #making image grayscale\n",
    "    img = cv2.equalizeHist(img) #Histogram equalization to enhance contrast\n",
    "    img = img/255 #normalizing\n",
    "    return img\n",
    "\n",
    "train_X = np.array(list(map(PreprocessImage, train_X)))\n",
    "test_X = np.array(list(map(PreprocessImage, test_X)))\n",
    "valid_X= np.array(list(map(PreprocessImage, valid_X)))\n",
    "\n",
    "#Reshaping the images\n",
    "train_X = train_X.reshape(train_X.shape[0], train_X.shape[1], train_X.shape[2],1)\n",
    "test_X = test_X.reshape(test_X.shape[0], test_X.shape[1], test_X.shape[2],1)\n",
    "valid_X = valid_X.reshape(valid_X.shape[0], valid_X.shape[1], valid_X.shape[2],1)\n",
    "\n",
    "#Augmentation\n",
    "image_data_generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2, shear_range=0.1, rotation_range=10)\n",
    "image_data_generator.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of the labels\n",
    "\n",
    "train_y = to_categorical(train_y, num_classes)\n",
    "test_y = to_categorical(test_y, num_classes)\n",
    "valid_y = to_categorical(valid_y, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL BUILDING\n",
    "\n",
    "**For this Model, we will build a convolutional neural network.**\n",
    "* Initialising the Convnet\n",
    "* Defining by adding layers\n",
    "* Compiling the Convnet\n",
    "* Train the Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 60)        1560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 60)        90060     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 60)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 30)        16230     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 30)        8130      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 30)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 8, 30)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1920)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               960500    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,081,490\n",
      "Trainable params: 1,081,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 00:09:15.039571: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "#Creating a Neural Network\n",
    "\n",
    "tf_cnn_model = Sequential()\n",
    "\n",
    "tf_cnn_model.add((Conv2D(60,(5,5),input_shape=(32, 32, 1) ,padding = 'Same' ,activation='relu')))\n",
    "tf_cnn_model.add((Conv2D(60, (5,5),padding=\"same\",activation='relu')))\n",
    "tf_cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#tf_cnn_model.add(Dropout(0.25))\n",
    "\n",
    "tf_cnn_model.add((Conv2D(30, (3,3),padding=\"same\", activation='relu')))\n",
    "tf_cnn_model.add((Conv2D(30, (3,3), padding=\"same\", activation='relu')))\n",
    "tf_cnn_model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "tf_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "tf_cnn_model.add(Flatten())\n",
    "tf_cnn_model.add(Dense(500,activation='relu'))\n",
    "tf_cnn_model.add(Dropout(0.5))\n",
    "tf_cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "tf_cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 00:12:34.996858: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 - 56s - loss: 0.9485 - accuracy: 0.6737 - val_loss: 0.1125 - val_accuracy: 0.9591 - 56s/epoch - 281ms/step\n",
      "Epoch 2/30\n",
      "200/200 - 56s - loss: 0.2728 - accuracy: 0.9138 - val_loss: 0.0596 - val_accuracy: 0.9839 - 56s/epoch - 278ms/step\n",
      "Epoch 3/30\n",
      "200/200 - 54s - loss: 0.1877 - accuracy: 0.9401 - val_loss: 0.0571 - val_accuracy: 0.9834 - 54s/epoch - 270ms/step\n",
      "Epoch 4/30\n",
      "200/200 - 65s - loss: 0.1461 - accuracy: 0.9556 - val_loss: 0.0333 - val_accuracy: 0.9881 - 65s/epoch - 327ms/step\n",
      "Epoch 5/30\n",
      "200/200 - 66s - loss: 0.1295 - accuracy: 0.9597 - val_loss: 0.0525 - val_accuracy: 0.9839 - 66s/epoch - 329ms/step\n",
      "Epoch 6/30\n",
      "200/200 - 63s - loss: 0.1065 - accuracy: 0.9688 - val_loss: 0.0326 - val_accuracy: 0.9917 - 63s/epoch - 317ms/step\n",
      "Epoch 7/30\n",
      "200/200 - 63s - loss: 0.1095 - accuracy: 0.9668 - val_loss: 0.0857 - val_accuracy: 0.9777 - 63s/epoch - 313ms/step\n",
      "Epoch 8/30\n",
      "200/200 - 72s - loss: 0.1025 - accuracy: 0.9691 - val_loss: 0.0343 - val_accuracy: 0.9902 - 72s/epoch - 358ms/step\n",
      "Epoch 9/30\n",
      "200/200 - 69s - loss: 0.0882 - accuracy: 0.9735 - val_loss: 0.0314 - val_accuracy: 0.9907 - 69s/epoch - 345ms/step\n",
      "Epoch 10/30\n",
      "200/200 - 63s - loss: 0.0983 - accuracy: 0.9708 - val_loss: 0.0281 - val_accuracy: 0.9896 - 63s/epoch - 315ms/step\n",
      "Epoch 11/30\n",
      "200/200 - 57s - loss: 0.0862 - accuracy: 0.9737 - val_loss: 0.0330 - val_accuracy: 0.9891 - 57s/epoch - 286ms/step\n",
      "Epoch 12/30\n",
      "200/200 - 56s - loss: 0.0876 - accuracy: 0.9748 - val_loss: 0.0378 - val_accuracy: 0.9902 - 56s/epoch - 280ms/step\n",
      "Epoch 13/30\n",
      "200/200 - 61s - loss: 0.0833 - accuracy: 0.9740 - val_loss: 0.0267 - val_accuracy: 0.9927 - 61s/epoch - 303ms/step\n",
      "Epoch 14/30\n",
      "200/200 - 60s - loss: 0.0799 - accuracy: 0.9761 - val_loss: 0.0371 - val_accuracy: 0.9871 - 60s/epoch - 298ms/step\n",
      "Epoch 15/30\n",
      "200/200 - 57s - loss: 0.0851 - accuracy: 0.9732 - val_loss: 0.0246 - val_accuracy: 0.9927 - 57s/epoch - 287ms/step\n",
      "Epoch 16/30\n",
      "200/200 - 68s - loss: 0.0726 - accuracy: 0.9795 - val_loss: 0.0315 - val_accuracy: 0.9881 - 68s/epoch - 339ms/step\n",
      "Epoch 17/30\n",
      "200/200 - 68s - loss: 0.0690 - accuracy: 0.9798 - val_loss: 0.0343 - val_accuracy: 0.9896 - 68s/epoch - 338ms/step\n",
      "Epoch 18/30\n",
      "200/200 - 67s - loss: 0.0740 - accuracy: 0.9791 - val_loss: 0.0401 - val_accuracy: 0.9891 - 67s/epoch - 337ms/step\n",
      "Epoch 19/30\n",
      "200/200 - 66s - loss: 0.0701 - accuracy: 0.9808 - val_loss: 0.0381 - val_accuracy: 0.9912 - 66s/epoch - 332ms/step\n",
      "Epoch 20/30\n",
      "200/200 - 60s - loss: 0.0740 - accuracy: 0.9788 - val_loss: 0.0358 - val_accuracy: 0.9922 - 60s/epoch - 301ms/step\n",
      "Epoch 21/30\n",
      "200/200 - 70s - loss: 0.0696 - accuracy: 0.9817 - val_loss: 0.0211 - val_accuracy: 0.9933 - 70s/epoch - 349ms/step\n",
      "Epoch 22/30\n",
      "200/200 - 71s - loss: 0.0673 - accuracy: 0.9797 - val_loss: 0.0269 - val_accuracy: 0.9922 - 71s/epoch - 353ms/step\n",
      "Epoch 23/30\n",
      "200/200 - 76s - loss: 0.0712 - accuracy: 0.9803 - val_loss: 0.0217 - val_accuracy: 0.9938 - 76s/epoch - 382ms/step\n",
      "Epoch 24/30\n",
      "200/200 - 67s - loss: 0.0606 - accuracy: 0.9834 - val_loss: 0.0298 - val_accuracy: 0.9927 - 67s/epoch - 336ms/step\n",
      "Epoch 25/30\n",
      "200/200 - 73s - loss: 0.0588 - accuracy: 0.9838 - val_loss: 0.0302 - val_accuracy: 0.9902 - 73s/epoch - 366ms/step\n",
      "Epoch 26/30\n",
      "200/200 - 76s - loss: 0.0705 - accuracy: 0.9818 - val_loss: 0.0253 - val_accuracy: 0.9917 - 76s/epoch - 378ms/step\n",
      "Epoch 27/30\n",
      "200/200 - 65s - loss: 0.0750 - accuracy: 0.9809 - val_loss: 0.0476 - val_accuracy: 0.9933 - 65s/epoch - 327ms/step\n",
      "Epoch 28/30\n",
      "200/200 - 62s - loss: 0.0754 - accuracy: 0.9798 - val_loss: 0.0372 - val_accuracy: 0.9943 - 62s/epoch - 309ms/step\n",
      "Epoch 29/30\n",
      "200/200 - 53s - loss: 0.0712 - accuracy: 0.9814 - val_loss: 0.0193 - val_accuracy: 0.9948 - 53s/epoch - 267ms/step\n",
      "Epoch 30/30\n",
      "200/200 - 63s - loss: 0.0657 - accuracy: 0.9815 - val_loss: 0.0253 - val_accuracy: 0.9943 - 63s/epoch - 315ms/step\n"
     ]
    }
   ],
   "source": [
    "#Compiling the model\n",
    "\n",
    "rms_Optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon = 1e-08)\n",
    "tf_cnn_model.compile(optimizer=rms_Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Fit the model\n",
    "\n",
    "historical_data = tf_cnn_model.fit(image_data_generator.flow(train_X, train_y, batch_size=32),\n",
    "                              epochs = 30, validation_data = (valid_X, valid_y),\n",
    "                              verbose = 2, steps_per_epoch= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_cnn_model.save('./output/model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score =  0.010815313085913658\n",
      "Test Accuracy = 0.998031497001648\n"
     ]
    }
   ],
   "source": [
    "# Testing the tf_cnn_model on the test set\n",
    "\n",
    "score = tf_cnn_model.evaluate(test_X, test_y, verbose=0)\n",
    "print('Test Score = ',score[0])\n",
    "print('Test Accuracy =', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
